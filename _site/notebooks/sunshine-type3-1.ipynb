{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport subprocess\n\nimport pandas as pd\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.models import resnet18\nfrom torch.utils.data import DataLoader, Dataset\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-09T03:41:16.542747Z","iopub.execute_input":"2023-11-09T03:41:16.543043Z","iopub.status.idle":"2023-11-09T03:41:20.196635Z","shell.execute_reply.started":"2023-11-09T03:41:16.543013Z","shell.execute_reply":"2023-11-09T03:41:20.195704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Use GPU only","metadata":{}},{"cell_type":"code","source":"if DEVICE != 'cuda':\n    raise RuntimeError('Make sure you have added an accelerator to your notebook; the submission will fail otherwise!')","metadata":{"execution":{"iopub.status.busy":"2023-11-09T03:41:26.149764Z","iopub.execute_input":"2023-11-09T03:41:26.150285Z","iopub.status.idle":"2023-11-09T03:41:26.155Z","shell.execute_reply.started":"2023-11-09T03:41:26.150255Z","shell.execute_reply":"2023-11-09T03:41:26.153915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helper functions for loading hidden dataset\n\n- Location of dataset : /kaggle/input/neurips-2023-machine-unlearning/\n- Contents of each record: Image, Image ID, Age roup (Target), Age, Person ID\n- retain.csv, forget.csv, validation.csv provided by competition\n- Use dataset loader with shuffle=True (to include randomness between different runs - 512 different checkpoints)","metadata":{}},{"cell_type":"code","source":"\ndef load_example(df_row):\n    \n    image = torchvision.io.read_image(df_row['image_path'])\n    # For each person, the following information is available\n    result = {\n        'image': image,\n        'image_id': df_row['image_id'],\n        'age_group': df_row['age_group'],\n        'age': df_row['age'],\n        'person_id': df_row['person_id']\n    }\n    return result\n\n\nclass HiddenDataset(Dataset):\n\n    def __init__(self, split='train'):\n        \n        super().__init__()\n        \n        self.examples = []\n        # location of Dataset + type of data\n        df = pd.read_csv(f'/kaggle/input/neurips-2023-machine-unlearning/{split}.csv')\n        \n        # Using Image IDs, retrieve images\n        df['image_path'] = df['image_id'].apply(\n            lambda x: os.path.join('/kaggle/input/neurips-2023-machine-unlearning/', 'images', x.split('-')[0], x.split('-')[1] + '.png'))\n        df = df.sort_values(by='image_path')\n        \n        # Split records for each individual\n        df.apply(lambda row: self.examples.append(load_example(row)), axis=1)\n        \n        if len(self.examples) == 0:\n            raise ValueError('No examples.')\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, idx):\n        example = self.examples[idx]\n        image = example['image']\n        image = image.to(torch.float32)\n        example['image'] = image\n        return example\n\n\ndef get_dataset(batch_size):\n    \n    # Load data for Retain, Forget and Validation datasets\n    retain_ds = HiddenDataset(split='retain')\n    forget_ds = HiddenDataset(split='forget')\n    val_ds = HiddenDataset(split='validation')\n\n    # Use dataloader to save RAM\n    retain_loader = DataLoader(retain_ds, batch_size=batch_size, shuffle=True)\n    forget_loader = DataLoader(forget_ds, batch_size=batch_size, shuffle=True)\n    validation_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n\n    return retain_loader, forget_loader, validation_loader","metadata":{"execution":{"iopub.status.busy":"2023-11-09T03:41:29.561611Z","iopub.execute_input":"2023-11-09T03:41:29.562052Z","iopub.status.idle":"2023-11-09T03:41:29.574875Z","shell.execute_reply.started":"2023-11-09T03:41:29.562001Z","shell.execute_reply":"2023-11-09T03:41:29.573816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Unlearning operation\nStrategy 3: Based on paper 'Selective forgetting in Deep Networks' by A. Golatkar et.al","metadata":{}},{"cell_type":"code","source":"pip install pyhessian","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Version 1\nimport pyhessian\nimport copy\n\ndef get_mean_var(p,hessian, alpha=3e-6):\n        \n    var = copy.deepcopy(1./(hessian+1e-8))\n    var = var.clamp(max=1e3)   \n    var = alpha * var\n\n    mu = copy.deepcopy(p.data0.clone())\n    return mu, var\n\ndef unlearning(\n    net, \n    retain_loader, \n    forget_loader, \n    val_loader):\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=0.005,\n                      momentum=0.9, weight_decay=5e-4)\n\n    for p in itertools.chain(net.parameters()):\n        p.data0 = copy.deepcopy(p.data.clone())\n        \n    hessian_accumulator = torch.zeros_like(model.parameters())\n    net.train()\n\n    for sample in retain_loader:\n        \n        inputs = sample[\"image\"]\n        targets = sample[\"age_group\"]\n        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n        \n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, targets)\n        \n        loss.backward()\n        optimizer.step()\n        \n        # Compute Hessian for current batch\n        batch_hessian = pyhessian.hessian(model, inputs)\n\n        # Accumulate Hessian contributions\n        hessian_accumulator += batch_hessian\n        \n    # Compute overall Hessian\n    overall_hessian = hessian_accumulator / len(retain_loader)    \n    \n    alpha = 1e-6\n    torch.manual_seed(1756)\n    for i, p in enumerate(net.parameters()):\n        mu, var = get_mean_var(p, overall_hessian, alpha=alpha)\n        p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n    \n    net.eval()\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-09T03:41:33.749164Z","iopub.execute_input":"2023-11-09T03:41:33.749839Z","iopub.status.idle":"2023-11-09T03:41:33.757658Z","shell.execute_reply.started":"2023-11-09T03:41:33.749807Z","shell.execute_reply":"2023-11-09T03:41:33.756443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# version 2\nimport copy\nimport itertools\nfrom tqdm import tqdm\n\ndef get_mean_var(p, is_base_dist=False, alpha=3e-6):\n        \n    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n    \n    var = var.clamp(max=1e3)\n    \n    var = alpha * var\n\n    mu = copy.deepcopy(p.data0.clone())\n\n\ndef unlearning(\n    net, \n    retain_loader, \n    forget_loader, \n    val_loader):\n\n    # netf = copy.deepcopy(net)\n    for p in itertools.chain(net.parameters()):\n        p.data0 = copy.deepcopy(p.data.clone())\n    \n    net.train()\n    loss_fn = nn.CrossEntropyLoss()\n\n    for p in net.parameters():\n        p.grad2_acc = 0\n        \n    \n    for sample in tqdm(retain_loader):\n\n        data = sample[\"image\"]\n        orig_target = sample[\"age_group\"]\n        \n        data, orig_target = data.to(DEVICE), orig_target.to(DEVICE)\n        \n        output = net(data)\n        \n        prob = torch.nn.functional.softmax(output, dim=-1).data\n\n        for y in range(output.shape[1]):\n            \n            target = torch.empty_like(orig_target).fill_(y)\n            \n            loss = loss_fn(output, target)\n            \n            net.zero_grad()\n            \n            loss.backward(retain_graph=True)\n            \n            for p in net.parameters():\n                if p.requires_grad:\n                    p.grad2_acc += (prob[:, y] * p.grad.data.pow(2))\n\n                    \n    for p in net.parameters():\n        \n        p.grad2_acc /= len(retain_loader)\n\n        \n    alpha = 1e-6\n    torch.manual_seed(1756)\n    for i, p in enumerate(net.parameters()):\n        mu, var = get_mean_var(p, False, alpha=alpha)\n        p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n    \n    net.eval()\n    return net","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Access dataset, load model, call unlearning function and generate submission file with unlearned model","metadata":{}},{"cell_type":"code","source":"# dummy pathway for local - does not exist in submission\nif os.path.exists('/kaggle/input/neurips-2023-machine-unlearning/empty.txt'):\n    subprocess.run('touch submission.zip', shell=True)\n    \nelse:\n    # tmp directory - cannot save in home dir\n    os.makedirs('/kaggle/tmp', exist_ok=True)\n    # batch size - 128\n    retain_loader, forget_loader, validation_loader = get_dataset(1)\n    # load model template\n    net = resnet18(weights=None, num_classes=10)\n    net.to(DEVICE)\n    # load model and call unlearning function 512 times\n    for i in range(512):\n        net.load_state_dict(torch.load('/kaggle/input/neurips-2023-machine-unlearning/original_model.pth'))\n        unlearning(net, retain_loader, forget_loader, validation_loader)\n        state = net.state_dict()\n        # save as checkpoint\n        torch.save(state, f'/kaggle/tmp/unlearned_checkpoint_{i}.pth')\n        \n    # Ensure that submission.zip will contain exactly 512 checkpoints \n    # (if this is not the case, an exception will be thrown).\n    unlearned_ckpts = os.listdir('/kaggle/tmp')\n    if len(unlearned_ckpts) != 512:\n        raise RuntimeError('Expected exactly 512 checkpoints. The submission will throw an exception otherwise.')\n    # zip it and create submission\n    subprocess.run('zip submission.zip /kaggle/tmp/*.pth', shell=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T03:41:36.670342Z","iopub.execute_input":"2023-11-09T03:41:36.671223Z","iopub.status.idle":"2023-11-09T03:41:36.685047Z","shell.execute_reply.started":"2023-11-09T03:41:36.67119Z","shell.execute_reply":"2023-11-09T03:41:36.684142Z"},"trusted":true},"execution_count":null,"outputs":[]}]}